#Version date 19.5.2020:
import numpy as np
import scipy
import matplotlib.pyplot as plt
plt.rcParams['figure.dpi'] = 300 #set plot dpi to 300
import numpy.random as ran
import scipy.stats as stats
from itertools import dropwhile
import entropy_estimators as ee
import os
import random

#Class for creating objects to handle data and calculate pdfs and mutual information
class My_MI:
    def __init__(self,ulff,dates,power,fesaf,Lshells,fluxes):
        #ULF data
        self.dates=dates
        self.power=power
        #FESA data
        self.Lshells=Lshells
        self.fluxes=fluxes
        self.energy=self._find_energy(fesaf)
        #filenames
        self.fesaname=fesaf
        self.ulfname=ulff
        self.energy=self._find_energy(fesaf)
        #Average fluxes
        self.avg_flux=self._L_avg(fluxes,0)
               
    def _nbins(self,x):
    #Function to define the number of bins for functions _createpdf and _jointpdf
        iqr=stats.iqr(x)
        lx=len(x)
        if np.all(np.floor(x)==np.ceil(x)) and lx==1: #Check if the vector has only constant values
            bins=np.min(x)
        else:
            #bins=int(np.floor(lx**(1/3))) #cube-root of number of data points
            #remove comments below to use Freedman-Doaconis instead
            xma=np.max(x)
            xmi=np.min(x)
            bins=((xma-xmi)*lx**(1/3))/(2*iqr)    #Freedman-Diaconis rule
            bins=int(np.ceil(bins))
        #print(str(len(x))+" data points,"+str(bins)+" bins")
        return 50
    
    def bin_ind(self,x,mini,maxi,steps):
    #Return bin index of x
        step=(maxi-mini)/steps
        rem1=x-mini #Get remainder
        S1=np.floor(rem1/step) #Number of full steps from mini is the index of the respective bin
        S1=int(S1)
        if S1==steps: #Adress the maximum value in data and insert it into the largest bin
            S1=S1-1
        return S1 #Determine the bin to which the ii entry in x belongs to 
    
    
    def _N_bins(self,x):
    #Assign number of occurrences to each bin
        mini=np.min(x)
        maxi=np.max(x)
        steps=self._nbins(x)
        N=np.zeros(steps)
        for ii in range(len(x)):
            S=self.bin_ind(x[ii],mini,maxi,steps)
            N[S]=N[S]+1 #Increase the number of occurrences in the bin by 1
        return N #Return the vector containing the occurrences per bin
    
    def _createpdf(self,x):
    #The function creates a probability density function of given data
    #based on the partition, which is the number of brackets
    #It is assumed that the data is a function of an underlying variable, e.g. time, 
    #and the data points are in increasing order of the underlying variable
    #Set minimum, maximum, stepsize and array for calculating number of occurrences of each bracket 
        #print("Calculating pdf")
        #import pdb; pdb.set_trace()
        lx=len(x)
        mini=np.min(x)
        maxi=np.max(x)
        partition=self._nbins(x)
        step=(maxi-mini)/partition
        if step==0:
            step=1 #Address vectors with constant values only
    
        N=self._N_bins(x)         
        #Normalize probabilities
        pdf=N/lx
            
        #REMOVE COMMENTS IF PLOTS ARE DESIRED
        brackets=np.linspace(mini+step/2,maxi-step/2,num=partition) #For plotting only, set mid-point of each bin
        #bracketslim=np.linspace(mini,maxi,num=partition+1) #"Walls" of the bins
        #fig=plt.figure()
        #ax=fig.gca()
        #ax.set_xticks(np.arange(mini,maxi,partition+1)) #Set vertical lines to illustrate brackets
        #ax.plot(brackets,N,'ro')
        #plt.title("Number of events per bracket")
        #plt.grid(True)
        #plt.show()
    
        #Plot distribution
        #plt.plot(brackets,pdf,'ro')
        #text="Code created pdf with",partition,"brackets for",len(x),"data points"
        #plt.title(text)
        #plt.ylim(min(pdf),max(pdf))
        #plt.show()
    
        #print("The marginal pdf is")
        #print(pdf)
        return pdf
    
    def _find_energy(self,filen):
    #Returns the energy level given in a FESA file name
        begin=filen.find("MeV", 0, len(filen))
        end=filen.find("MeV", 0, len(filen))
        energy=filen[begin-3:begin]
        return energy
    
    def _jointpdf(self,x,y):
    #The function creates a joint probability density function of two given data sets.
    #Set minimum, maximum, stepsize and array for calculating number of occurrences of each bin 
        #print("Calculating joint pdf")
        mini1=np.min(x)
        maxi1=np.max(x)
        mini2=np.min(y)
        maxi2=np.max(y)
        part1=self._nbins(x) #partitions
        part2=self._nbins(y)
        step1=(maxi1-mini1)/part1
        step2=(maxi2-mini2)/part2
        lx=len(x)
        ly=len(y)
        if step1==0:
            step1=1 #Address vectors with constant values only
        if step2==0:
            step2=1 #Address vectors with constant values only
        bracs1=np.zeros(lx) #Bin indices for data1
        bracs2=np.zeros(ly) #Bin indices for data2
        pdf=np.zeros([part1,part2]) #Matrix for joint probabilities
    
        #Assign bin indices
        for ii in range(lx):
            bracs1[ii]=self.bin_ind(x[ii],mini1,maxi1,part1)
        for jj in range(ly):    
            bracs2[jj]=self.bin_ind(y[jj],mini2,maxi2,part2)
    
        #Populate pdf
        Total=0
        for ii in range(0,part1):
            test1=1*(bracs1==ii)
            for jj in range(0,part2):
                test2=1*(bracs2==jj)
                summa=np.sum(test1.dot(test2))
                Total=Total+summa
                #import pdb; pdb.set_trace()
                pdf[ii][jj]=summa
        pdf=pdf/Total #Normalize
        #print("The sum of the joint probabilities is equal to", np.sum(pdf))
        #print(pdf)
        return pdf
        
    def _mutualinfo(self,x,y):
    #Calculate the mutual information based on two given data sets
        pdfX=self._createpdf(x)
        pdfY=self._createpdf(y)
        pdfXY=self._jointpdf(x,y)
        I=0
        #import pdb; pdb.set_trace()
        for ii in range(len(pdfX)):
            for jj in range(len(pdfY)):
                #Handle division by zero and other exceptions
                if (pdfX[ii]*pdfY[jj])==0 and pdfXY[ii][jj]>0:
                    I=100 #This is a convention, see "Elements of Infomation Theory" OR SET TO SOME LARGE FINITE NUMBER
                    return I
                elif (pdfX[ii]*pdfY[jj])==0 and pdfXY[ii][jj]==0:
                    In=0 #ibid.
                elif (pdfX[ii]*pdfY[jj])!=0 and pdfXY[ii][jj]==0:
                    In=0 #ibid.
                else:
                    In=pdfXY[ii][jj]*np.log2(pdfXY[ii][jj]/(pdfX[ii]*pdfY[jj]))
                I=I+In
        #import pdb; pdb.set_trace()
        #print("Mutual info is equal to", I)
        return I
    
    
    def _t_offset(self,pwr,flux,t):
    #Function for creating time offset btw. power and e fluxes
    #Positive offset means that the power preceeds the e flux
    #t is the number of 9 hour steps to be offset
        lp=len(pwr)
        lf=len(flux)
        #import pdb; pdb.set_trace()
        if t>=0:
            #import pdb; pdb.set_trace()
            off_pwr=pwr[t:]
            off_flux=flux[0:lp-t]
        if t<0:
            off_pwr=pwr[0:lp+t]
            off_flux=flux[-t:]
        #print("Power preceeds flux by",t,"time steps")
        return off_pwr, off_flux, t

    def _remnans(self,ulf,fesa):
    #Takes in ULF and FESA data
    #Removes the "nan" values in the FESA data and returns two arrays, where the nans have been removed from 
    #the FESA data and the corresponding ULF values have also been removed, so that the time stamps stay
    #in on-to-one correspondence. Also returns the number of removed nans.
        nflx=[]
        nulf=[]
        for ii in range(len(fesa)):
            if fesa[ii]=="nan" or fesa[ii] is np.nan or fesa[ii] is float('nan') or np.isnan(fesa[ii]):
                continue
            else:
                nflx.append(np.float64(fesa[ii]))
                nulf.append(np.float64(ulf[ii]))
        #print("Removed",len(fesa)-len(nflx),"nans")
        return np.asarray(nulf),np.asarray(nflx)

    def _tremnans(self,ulf,flx,toff):
    #The function creates the desired time step offset in the ULF and FESA data
    #and then removes the nans values from the FESA data and also amends the ULF data accordingly.    
        npwr,nflx,t=self._t_offset(ulf,flx,toff)
        npwr1,nflx1=self._remnans(npwr,nflx)
        return npwr1,nflx1,t
    
    def _L_avg(self,x,ax):
    #Average e fluxes over all L shells
        avg_flux=np.nanmean(x,axis=ax)
        return avg_flux
    
    def _mi_toff_corr(self,x,y,tau):
    #Calculate the mutual info for the desired time offsets
        tstep=9 #Hard coded size of a single time step. The other functions use the number of steps, i.e. tausteps
        tausteps=np.linspace(-tau,tau,2*tau+1,dtype=np.int8)
        mi=np.empty([len(tausteps),4]) #Array to store mutual info etc.
        for jj in range(len(tausteps)):
            #import pdb; pdb.set_trace()
            nx,ny,tt=self._tremnans(x,y,tausteps[jj])
            #import pdb; pdb.set_trace()
            #minflux=min(minflux,min(ny)) #Can be used, if minflux and pinpwr need to be checked
            #minpwr=min(minpwr,min(nx))
            mi[jj,0:2]=np.array([tt*tstep,self._mutualinfo(nx,ny)])
            #print(mi1)
            _corr=stats.pearsonr(nx,ny)
            mi[jj,2:4]=_corr
        return mi
        
#Utility functions
#Functions for reading in data, writing into file and saving figures
def new_comment(f,txt):
#Write txt as a comment line in an open file f
    _txt="#"+txt+"\n"
    f.write(_txt)
    
def is_comment(s):
    #function to check if a line starts with some character, here "#" for comment
    # return true if a line starts with #
    return s.startswith('#')

def readerULF(name):
#This function reads ULF data from a .txt file into an array
    file = open(name, 'r')
    fname=name
    arr1=[]
    arr2=[]
    np.set_printoptions(precision=16) #For checking manually that input data is not rounded
    
    for line in dropwhile(is_comment,file):
        pl=line.split()
        arr1.append(np.unicode_(pl[0]))
        arr2.append(np.float64(pl[1]))               
    #Create a record of the arrays
    rec=np.rec.fromarrays((arr1, arr2), names=('dates', 'power'))
    file.close()
    return rec,fname

def readerFESA(name):
#This function reads FESA data from a .txt file into an array
    file = open(name, 'r')
    fname=name
    arr1=[]
    arr2=[]
    np.set_printoptions(precision=16) #For checking manually that input data is not rounded
    
    for line in dropwhile(is_comment,file):
        pl=line.split()
        arr1.append(np.float32(pl[0])) #L bin
        arr2.append(np.float64(pl[1:]))#The row of flux values per time stampt for the respective L bin              
    file.close()
    arr1=np.asarray(arr1)
    arr2=np.asarray(arr2)
    return arr1, arr2, fname

def finder(name1):
#Finds the FESA energy range provided in the file name
    word1="MeV"
    lword1=len(word1)
    ind=name1.find(word1)
    res=name1[ind-3:ind+lword1]
    import pdb; pdb.set_trace()
    return res

def figstodir(fesafile):
    directory = "plots/FESA"+finder(fesafile).replace(".","_")   
    parent_dir = "/Users/mikkosavola/Documents/Opiskelu_HY/Kandidaatintutkielma" #Parent Directory path 
    path = os.path.join(parent_dir, directory)  # Path 
  
    try:
        os.makedirs(path) #Create directory
        print("Directory '% s' created" % directory) 
        return path
    except OSError as error:
        print("Folder",path,"exists already")
        return path
    
def transf_name(_ufunc):
    #Returns the "name" of a utility function ufunc when given str(ufunc) as input
        n_ufunc=str(_ufunc)
        begin=n_ufunc.find("<", 0, len(n_ufunc))
        end=n_ufunc.find(">", 0, len(n_ufunc))
        _name=n_ufunc[begin+1:end-1]
        return _name
    
def write_to_file(f,x):
#Writes vector x as lines into a file
    for ii in range(len(x)):
         f.write(str(x[ii])+"\n")
    print("Saved data into file")

def annot(xs,ys,zs,text):
    # zip joins x, y and z coordinates in triplets
    #xs and ys give the points to be annotated, zs contains the annortations and text is
    #the text to be added
    for x,y,z in zip(xs,ys,zs):
        label = str(text+" "+"{:.2f}".format(z))
        plt.annotate(label, # this is the text
                 (x,y), # this is the point to label
                 textcoords="offset points", # how to position the text
                 xytext=(0,10), # distance from text to points (x,y)
                 ha='center') # horizontal alignment can be left, right or center
